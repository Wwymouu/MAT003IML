{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3 - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are widely used and form the foundation of many advanced non-linear techniques such as support vector machines and neural networks. You propably have seen an image depicting linear regression already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, linear_model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "beta = 3\n",
    "\n",
    "x = 4*np.random.random(n)\n",
    "eps = 2*np.random.random(n)-1\n",
    "y = x*beta + eps\n",
    "\n",
    "plt.plot(x,y,'ob',[0,4],[0,4*beta],'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\mathcal{X} = ℝ^d$ and $\\mathcal{Y} = ℝ$. For linear regression, we assume that the data are of the form $$y \\approx \\sum_{i=1}^d β_i x^{(i)} + β_0.$$ Hence, we choose the hypothesis as the class of affine functions: $$\\{x\\mapsto \\langleβ,x\\rangle + β_0 \\;\\vert\\; β_0,…,β_d ∈ℝ\\}.$$\n",
    "\n",
    "Usually, it is more convenient to incorporate the bias $β_0$ into the variables $β$ as well. To do so, denote $x_0 = 1$ and consider instead $\\mathcal{X} = ℝ^{d+1}$ with $x = (1,x^{(i)},…,x^{(i)})$. We will do so implicitly and hence will work with the hypothesis class of linear functions instead:\n",
    "$$\\mathcal{F} = \\{x\\mapsto \\langleβ,x\\rangle \\;\\vert\\; β∈ℝ^{d+1}\\}$$\n",
    "\n",
    "Which loss function should we choose? It is common to consider one of the following two:\n",
    "- squared error (OLS) $L(y,y') = (y−y')^2$\n",
    "- absolute value loss function $L(y,y') = |y−y'|$\n",
    "\n",
    "Don't use whatever loss function comes to your mind if there is no need for it! Stay with the well-known choices.\n",
    "\n",
    "Denote $X = (x_i^{(j)})_{i,j}$ the matrix with rows given by $x_1,…,x_n$. Then, we and up with the following problem: Find $\\operatorname{argmin}_{h∈\\mathcal{F}} \\hat{R}(h)$ or more explicitely\n",
    "* $\\operatorname{argmin}\\{ \\frac{1}{n} \\sum_{i=1}^n (y_i − \\langleβ,x_i\\rangle)^2 \\;\\vert\\; β∈ℝ^d\\} = \\operatorname{argmin}\\frac{1}{n} \\|Xβ−y\\|_2^2$, resp.\n",
    "* $\\operatorname{argmin}\\{ \\frac{1}{n} \\sum_{i=1}^n |y_i − \\langleβ,x_i\\rangle| \\;\\vert\\; β∈ℝ^d\\} = \\operatorname{argmin}\\frac{1}{n} \\|Xβ−y\\|_1$\n",
    "\n",
    "Advantages:\n",
    "- Easy to understand model\n",
    "- Scale well on large datasets\n",
    "- Work well with sparse data\n",
    "- Quite stable for small changes in the input data\n",
    "- Rarely overfits (low variance)\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "- Linear model might not represent the truth\n",
    "- Tend to underfit (high bias)\n",
    "- Cannot treat well strongly correlated features e.g. $x_1 = −x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## The algorithm - by foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present the algorithms on the Boston house price data set from scikit-learn. Throughout the whole part we will use the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.data.shape)\n",
    "print(boston.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = boston.data, boston.target\n",
    "y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is a bad idea to adhoc start with your machine learning algorithm. Instead we should first take care of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, data are not generated from an i.i.d. source as we always assume. Instead, the samples might be time-dependent without us noticing or are correlated in some other way among each other. Hence, it is always a good idea to shuffle them before usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    z = np.hstack((x, y))\n",
    "    np.random.shuffle(z)\n",
    "    return np.hsplit(z, [x.shape[1]])\n",
    "\n",
    "x, y = shuffle(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a need to normalize all features to let them live on the same scale, called feature scaling. In the special case of linear regression this is usually not needed. Though, some implementations of linear regression might still profit from data being not too large. Other algorithms cannot be run without normalization. Normalization usually means\n",
    "$$x_{new} = \\frac{x_{old}−x_{mean}}{x_{sd}}$$\n",
    "and\n",
    "$$y_{new} = y_{old} − y_{mean}.$$\n",
    "Make sure that as soon as you normalized your data, you have to do the reverse once you want to interpret your results! For sake of an easier presentation, we omit normalization here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a result of how good our predictions will be we cannot plainly use the empirical error obtained. This error is biased as we trained the model to recreate the data presented. Instead, one typically calculates the empirical error on new, previously unseen data to obtain a better estimate on how good the model describes reality. This step is called testing. As a rule of thumb around 20% of your total data should be hold back for testing in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitting(x, y, test_size=0.2):\n",
    "    n = x.shape[0]\n",
    "    train_size = int(n * (1 - test_size))\n",
    "    return x[:train_size, ], x[train_size:, ], y[:train_size, ], y[train_size:, ]\n",
    "\n",
    "x_train, x_test, y_train, y_test = splitting(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's implement a function yielding the empirical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_empirical_risk(x, y, b):\n",
    "    return 1 / x.shape[0] * (np.linalg.norm(x.dot(b) - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Batch) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $β \\mapsto \\frac{1}{n} \\|Xβ−y\\|_2^2$ is convex. Hence, we can apply a standard gradient descent method to find its minimum. The gradient is given by\n",
    "$$\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2 = \\frac{2}{n}(X^t X β − X^t y).$$\n",
    "Fix some step length/learning rate $α∈ℝ_+$. Then, we iterate the step\n",
    "$$ β_{new} = β_{old} − \\frac{2α}{n}(X^t X β − X^t y)$$\n",
    "until the algorithm converges.\n",
    "\n",
    "What is the role of $α$? In theory, one can choose $α$ depending on the iteration step. A typical choice is $α = \\frac{(\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)^t (\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)}{(\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)^t x^t x (\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)}$ (Gauss-Newton algorithm). Another approach is to choose alpha in the beginning by hand. Then, one calls $α$ a hyperparameter (a parameter in your algorithm which is not learned). How to treat hyperparameters will be the content of a future class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient(x, y, alpha):\n",
    "    n, d = x.shape\n",
    "    result = np.zeros((d, 1))\n",
    "    cost_new = linear_regression_empirical_risk(x, y, result)\n",
    "    cost_old = 0\n",
    "    i = 0\n",
    "    while (cost_new - cost_old) ** 2 > 10 ** (-20):\n",
    "        gradient = (np.transpose(x)).dot(x.dot(result) - y)\n",
    "        alpha = (np.transpose(gradient).dot(gradient)) / (\n",
    "            (np.transpose(gradient).dot(np.transpose(x))).dot(x.dot(gradient)))\n",
    "        result -= (2 * alpha / n) * gradient\n",
    "        cost_new, cost_old = linear_regression_empirical_risk(x, y, result), cost_new\n",
    "        i += 1\n",
    "    print(\"Iterations: {}\".format(i))\n",
    "    return result\n",
    "\n",
    "beta_grad = linear_regression_gradient(x_train, y_train, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, a closed form solution for our minimization problem\n",
    "$$\\operatorname{argmin}_β \\frac{1}{n} \\|Xβ−y\\|_2^2 = \\frac{1}{n}(β^t X^t X β − 2β^t X^t y + y^t y)$$\n",
    "exists. The gradient is given by\n",
    "$$\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2 = \\frac{2}{n}(X^t X β − X^t y).$$\n",
    "This immediately yields\n",
    "$$β = (X^t X)^{−1} X^t y.$$\n",
    "In general, $X^t X$ must not be invertible. This is particularly the case if the number of features is high compared to the sample size. In such a case the equation for $β$ admits several solutions. One of them can be found by replacing the inverse matrix by a generalized inverse/pseudoinverse $X^+$. This leads to the solution\n",
    "$$β = X^+ y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_normal(x, y):\n",
    "    result = np.linalg.pinv(x).dot(y)\n",
    "    return result\n",
    "\n",
    "beta_normal = linear_regression_normal(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm - by scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using scikit-learn, life gets so much easier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, shuffle=False, test_size=0.2)\n",
    "lr = linear_model.LinearRegression(fit_intercept=False, normalize=False).fit(x_train, y_train)\n",
    "\n",
    "beta_sklearn = np.transpose(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments:\n",
    "* scikit-learn treats machine learning algorithms as classes. For example, .fit is a method of the class LinearRegression.\n",
    "* All derived information are stored in variables with a trailing underscore by scikit-learn.\n",
    "\n",
    "Scikit-learn has also implemented tools to predict data and calculate the mean squared error or mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lr.predict(x_test)\n",
    "metrics.mean_squared_error(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our different algorithms perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------Normal----------\")\n",
    "print(\"My estimated beta: {}\".format(beta_normal))\n",
    "print(\"Empirical error on training set (normal): {:.2f}\".format(\n",
    "    linear_regression_empirical_risk(x_train, y_train, beta_normal)))\n",
    "print(\"Empirical error on test set (normal): {:.2f}\".format(\n",
    "    linear_regression_empirical_risk(x_test, y_test, beta_normal)))\n",
    "\n",
    "print(\"----------Gradient----------\")\n",
    "print(\"My estimated beta: {}\".format(beta_grad))\n",
    "print(\"Empirical error on training set (grad): {:.2f}\".format(\n",
    "    linear_regression_empirical_risk(x_train, y_train, beta_grad)))\n",
    "print(\"Empirical error on test set (grad): {:.2f}\".format(linear_regression_empirical_risk(x_test, y_test, beta_grad)))\n",
    "\n",
    "print(\"----------Scikit-Learn----------\")\n",
    "print(\"lr.coef_: {}\".format(beta_sklearn))\n",
    "print(\"Empirical error on training set (sklearn): {:.2f}\".format(\n",
    "    linear_regression_empirical_risk(x_train, y_train, beta_sklearn)))\n",
    "print(\"Empirical error on test set (sklearn): {:.2f}\".format(\n",
    "    linear_regression_empirical_risk(x_test, y_test, beta_sklearn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides an additional value to get an idea of the performance of our machine learning algorithm, the so-called score $R^2$ (also known as explained variance or coefficent of determination). It is given by\n",
    "$$R^2 = (1-\\frac{R(β)}{\\operatorname{Var}(y)}).$$\n",
    "As the name suggests it provides the proportion of the variance in the dependent variable that is predictable from the independent variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(x_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this learning?\n",
    "* No iterative learning steps in normal equation does not mean that the algorithm is not learning.\n",
    "* Linear regression profits a lot from its easy form. This makes the algorithm explainable.\n",
    "\n",
    "Normal equation vs gradient descent\n",
    "* Calculating inverse matrix is difficult for many features. Updating rule stays easy.\n",
    "* Learning rate has to be chosen well for gradient descent to work. So the learning algorithm gets more difficult. --> See also Class 4\n",
    "\n",
    "Theoretical bounds\n",
    "* Consider a data set $(x^{(i)},y_i)_{i=1,…,n}$ following the model $y = x^t β + ε$ for i.i.d. noise $ε$ with mean 0 and variance $σ^2$.\n",
    "* The training error satisfies $\\mathbb{E}[R(β)] = σ^2 (1−\\frac{d+1}{n_{test}})$.\n",
    "* The test error satisfies $\\mathbb{E}[R(β)] = σ^2 (1+\\frac{d+1}{n_{train}})$.\n",
    "* This shows that the model is indeed learning some information from the noise as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the code yourself! Possible ideas that might lead you to interesting observations are:\n",
    "1. Compare all algorithms for the squarred error.\n",
    "2. Implement the normalization. How do the results of your algorithms change?\n",
    "3. Extend the algorithms to allow for affine hypotheses. How do the results of your algorithms change?\n",
    "2. Implement the algorithm for the absolute value loss function. Compare the estimator for the absolute value loss function with the one for the squared error.\n",
    "4. Try your algorithms on the new dataset diabetes from scikit-learn.\n",
    "5. Try your algorithm on a data set you generated yourself: Fix some $β∈ℝ^d$ with possibly some parameters being (close to) zero. Generate some samples via $y = \\sum_{i=1}^d β_i x^{(i)} + ε$ with some i.i.d. noise $ε$.\n",
    "6. Plot the training error and the test error for your model trained on smaller sized data sets of size $m\\le n$ for different $m$. Can you see how your model is “learning”?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
