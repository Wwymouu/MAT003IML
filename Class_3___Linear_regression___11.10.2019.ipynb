{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3 - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are widely used and form the foundation of many advanced non-linear techniques such as support vector machines and neural networks. You propably have seen an image depicting linear regression already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, linear_model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 30\n",
    "beta = 3\n",
    "\n",
    "x = 4*np.random.random(n)\n",
    "eps = 2*np.random.random(n)-1\n",
    "y = x*beta + eps\n",
    "\n",
    "plt.plot(x,y,'ob',[0,4],[0,4*beta],'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\mathcal{X} = ℝ^d$ and $\\mathcal{Y} = ℝ$. For linear regression, we assume that the data are of the form $$y \\approx \\sum_{j=1}^d β_j x^{(j)} + β_0.$$ Hence, we choose the hypothesis as the class of affine functions: $$\\{x\\mapsto \\langleβ,x\\rangle + β_0 \\;\\vert\\; β_0,…,β_d ∈ℝ\\}.$$\n",
    "\n",
    "Usually, it is more convenient to incorporate the bias $β_0$ into the variables $β$ as well. To do so, denote $x_0 = 1$ and consider instead $\\mathcal{X} = ℝ^{d+1}$ with $x = (1,x^{(1)},…,x^{(d)})$. We will do so explicitly later on and hence will work with the hypothesis class of linear functions instead:\n",
    "$$\\mathcal{F} = \\{x\\mapsto \\langleβ,x\\rangle \\;\\vert\\; β∈ℝ^{d+1}\\}$$\n",
    "\n",
    "Which loss function should we choose? It is common to consider one of the following two:\n",
    "- squared error (OLS) $L(y,y') = (y - y')^2$\n",
    "- absolute value loss function $L(y,y') = |y−y'|$\n",
    "\n",
    "where $y$ is the true label and $y'$ is our prediction of it.\n",
    "\n",
    "**Attention!** When choosing an error function, it is generally a good idea to stick with a few, usual, well-known choices. These two are most commonly used because of a lot of reasons coming from statistics (e.g. sparsity) and/or optimization (e.g. convexity).\n",
    "\n",
    "Denote $X = (x_i^{(j)})_{i,j}$ the matrix with rows given by $x_1,…,x_n$. Then, we end up with the following problem: Find $\\operatorname{argmin}_{h∈\\mathcal{F}} \\hat{R}(h)$ or more explicitely\n",
    "* $\\operatorname{argmin}\\{ \\frac{1}{n} \\sum_{i=1}^n (y_i − \\langleβ,x_i\\rangle)^2 \\;\\vert\\; β∈ℝ^d\\} = \\operatorname{argmin}\\frac{1}{n} \\|Xβ−y\\|_2^2$, resp.\n",
    "* $\\operatorname{argmin}\\{ \\frac{1}{n} \\sum_{i=1}^n |y_i − \\langleβ,x_i\\rangle| \\;\\vert\\; β∈ℝ^d\\} = \\operatorname{argmin}\\frac{1}{n} \\|Xβ−y\\|_1$\n",
    "\n",
    "Advantages:\n",
    "- Easy to understand model\n",
    "- Scale well on large datasets\n",
    "- Work well with sparse data\n",
    "- Quite stable for small changes in the input data\n",
    "- Rarely overfits (low variance)\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "- Linear model might not be realistic\n",
    "- Tendency to underfit (high bias)\n",
    "- Cannot treat well strongly correlated features e.g. $x_1 = −x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Linear regression: the algorithm (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present the algorithms on the Boston house price data set from scikit-learn. Throughout the whole part we will use the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "print(boston.data.shape)\n",
    "print(boston.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "x, y = boston.data, boston.target\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it is a bad idea to immediately start training your machine learning algorithm: we first need to __preprocess__ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, datapoints in a sample are not realizations of a perfectly i.i.d. sequence of random variables. Instead, the sample might be generated in such a way that the datapoints have some degree of time dependence without us noticing, or they might be correlated in some other way among each other. \n",
    "\n",
    "Moreover, some algorithms may depend on the order of the datapoints in the sample.\n",
    "\n",
    "In order to reduce these efffects as much as possible,it is always a good idea to shuffle the datapoints before usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1500)\n",
    "\n",
    "def shuffle(x, y):\n",
    "    z = np.hstack((x, y))\n",
    "    np.random.shuffle(z)\n",
    "    return np.hsplit(z, [x.shape[1]])\n",
    "\n",
    "x, y = shuffle(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.29400e-02 2.80000e+01 1.50400e+01 ... 1.82000e+01 3.96900e+02\n",
      "  1.05900e+01]\n",
      " [9.32909e+00 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.96900e+02\n",
      "  1.81300e+01]\n",
      " [2.89600e-01 0.00000e+00 9.69000e+00 ... 1.92000e+01 3.96900e+02\n",
      "  2.11400e+01]\n",
      " ...\n",
      " [2.18700e-02 6.00000e+01 2.93000e+00 ... 1.56000e+01 3.93370e+02\n",
      "  5.03000e+00]\n",
      " [5.02300e-02 3.50000e+01 6.06000e+00 ... 1.69000e+01 3.94020e+02\n",
      "  1.24300e+01]\n",
      " [9.60400e-02 4.00000e+01 6.41000e+00 ... 1.76000e+01 3.96900e+02\n",
      "  2.98000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a result of how good our model will be we cannot plainly use the empirical error obtained on the training dataset. This error is biased as we trained the model in such a way as to minimize it!\n",
    "\n",
    "Instead, one typically would like to compute the empirical error on new, previously unseen data to obtain a better estimate of how good the model describes reality. These \"new, previously unseen\" data must be set aside from the beginning and separated from the training dataset into a test dataset.\n",
    "\n",
    "The test dataset will not be used for training and we will use it only at end, in order to come up with an estimate of how good our model is.\n",
    "\n",
    "The process of evaluating the prediction error of our model using the empirical error on the test dataset is called testing. As a rule of thumb around 20% of your total data should be hold back for testing in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(x, y, test_size=0.2):\n",
    "    n = x.shape[0]\n",
    "    train_size = int(n * (1 - test_size))\n",
    "    return x[:train_size, ], x[train_size:, ], y[:train_size, ], y[train_size:, ]\n",
    "\n",
    "x_train, x_test, y_train, y_test = splitting(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (404, 13)\n",
      "y_train:  (404, 1)\n",
      "x_test:  (102, 13)\n",
      "y_test:  (102, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train: \", x_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"x_test: \", x_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passage from affine functions to linear ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, it is more convenient to incorporate the bias $β_0$ into the variables $β$ as well. To do so, denote $x_0 = 1$ and consider instead $\\mathcal{X} = ℝ^{d+1}$ with: $$x = (1,x^{(1)},…,x^{(d)}).$$ We will do so explicitly later on and hence will work with the hypothesis class of linear functions instead:\n",
    "$$\\mathcal{F} = \\{x\\mapsto \\langleβ,x\\rangle \\;\\vert\\; β∈ℝ^{d+1}\\}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 14)\n",
      "(102, 14)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.hstack((np.ones((x_train.shape[0],1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0],1)), x_test))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interlude: normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there is a need to normalize all features in order for them to live on the same scale. This procedure is called feature scaling or normalization. \n",
    "\n",
    "In the special case of linear regression this is usually not needed, even though some implementations of linear regression might still profit from data being not too large. There are algorithms that cannot be run without normalization, therefore it's important to understand this data preprocessing technique.\n",
    "\n",
    "Normalization means that we center and rescale all our datapoints, so that they have zero (sample) mean and unit (sample) variance. In the case of multi-dimensional features, the normalization is done component-wise:\n",
    "\n",
    "$$x_{new}^{(j)} = \\frac{x_{old}^{(j)}−x_{mean}^{(j)}}{x_{std}^{(j)}},$$\n",
    "\n",
    "while for labels the whole vector is normalized:\n",
    "\n",
    "$$y_{new} = \\frac{y_{old} − y_{mean}}{y_{std}}.$$\n",
    "\n",
    "**Attention!** If you decide to normalize, remember to normalize only your training dataset! Otherwise you will partially contaminate the test dataset by incorporating (some) information about its sample mean and variance.\n",
    "\n",
    "**Attention!** When you need to output your predictions (on the test dataset or outside of it) make sure that you reverse the normalization operation. Your model has learnt to predict from normalized input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not implement normalization today, but the commented code below can be used to normalize a **training dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# x_mean = np.mean(x_train, axis=0)\n",
    "# x_std = np.std(x_train, axis=0)\n",
    "\n",
    "# x_train = (x_train - x_mean)/x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[            nan  8.24423038e-17  7.33736504e-17  1.79944068e-15\n",
      " -1.37129032e-16 -2.83106871e-15  1.50604914e-15 -9.90956492e-16\n",
      "  3.77379646e-16 -6.56790354e-17  5.77096127e-17  2.16368452e-14\n",
      "  9.78782512e-15 -6.42775162e-16]\n",
      "[nan  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Check that all coordinates of the data matrix X have mean zero and variance one\n",
    "\n",
    "# print(np.mean(x_train, axis=0))\n",
    "# print(np.std(x_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_mean = np.mean(y_train)\n",
    "# y_std = np.std(y_train)\n",
    "\n",
    "# y_train = (y_train - y_mean)/y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that the label vector has now mean zero and variance one\n",
    "\n",
    "# print(np.mean(y_train))\n",
    "# print(np.std(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's implement a function that will allow us to compute the error according our definition of empirical error. Remember that we want to have mean squared loss. In other words, we want to evaluate the error between the real label vector $y$ and the predicted label vector $y'$ using the (squared) L^2 norm:\n",
    "\n",
    "$$ L(y,y') = \\frac{1}{n} \\sum_{i=1}^n (y_i - {y'}_i)^2. $$\n",
    "\n",
    "This function will be used to compute both the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_error(y_true, y_pred):\n",
    "    return (1/len(y_true))*sum((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Batch) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our strategy is to find the vector of coefficients, $\\beta$, that minimizes the empirical error on our dataset:\n",
    "\n",
    "$$\\operatorname{argmin}\\{ \\frac{1}{n} \\sum_{i=1}^n (y_i − \\langleβ,x_i\\rangle)^2 \\;\\vert\\; β∈ℝ^d\\} = \\operatorname{argmin}\\frac{1}{n} \\|Xβ−y\\|_2^2$$\n",
    "\n",
    "The function $β \\mapsto \\frac{1}{n} \\|Xβ−y\\|_2^2$ is convex and we may want to apply a standard gradient descent method to find its minimum. \n",
    "\n",
    "The gradient is given by\n",
    "$$\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2 = \\frac{2}{n}(X^t X β − X^t y).$$\n",
    "\n",
    "Fix some step length/learning rate $α∈ℝ_+$ and a starting estimate for $\\beta$ (we can take, for example, the origin). Then, we update our estimate by \"moving towards the minimum\":\n",
    "\n",
    "$$ β_{new} = β_{old} − \\frac{2α}{n}(X^t X β − X^t y)$$\n",
    "\n",
    "until the algorithm converges.\n",
    "\n",
    "What is the role of $α$? \n",
    "\n",
    "In theory, the smaller $\\alpha$ is, the smaller each step is, which means that you will be able to identify your point of minimum very precisely, but your algorithm will require a lot of time for convergence.\n",
    "\n",
    "The bigger $\\alpha$ is, the faster you can move towards the minimum, but you will identify it with less precision.\n",
    "\n",
    "In practice a variety of recipes can be used for choosing a \"good\" alpha. Ideally we would like $\\alpha$ to start big (so we can zoom fast towards the minimum) and then become smaller (so we can identify the point of minimum with more precision). In other words, one can choose $α$ depending on the iteration step. \n",
    "\n",
    "A typical choice (so called Gauss-Newton algorithm) is: \n",
    "\n",
    "$$α = \\frac{(\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)^t (\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)}{(\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)^t x^t x (\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2)}$$ \n",
    "\n",
    "Another approach is to fix an arbitrary $\\alpha$ from the beginning and treat it as a model parameter (called hyperparameter). Of course, this particular choice of $\\alpha$ will impact the efficiency of the minimization and in the course of our class we will study techniques (cross-validation for hyperparameter selection) that will allow us to learn how to select a good value for $\\alpha$, specifically suited for the problem we want to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_gradient_train(x, y, alpha=None):\n",
    "    \n",
    "    n, d = x.shape\n",
    "    result = np.zeros((d, 1))\n",
    "    cost_new = my_error(y, x.dot(result))\n",
    "    cost_old = 0\n",
    "    i = 0\n",
    "    while np.abs(cost_new - cost_old) > 10 ** (-6):\n",
    "        print(i)\n",
    "        print(cost_new)\n",
    "        gradient = (np.transpose(x)).dot(x.dot(result) - y)\n",
    "        alpha = (np.transpose(gradient).dot(gradient)) / (\n",
    "            (np.transpose(gradient).dot(np.transpose(x))).dot(x.dot(gradient)))\n",
    "        result -= (2 * alpha / n) * gradient\n",
    "        cost_new, cost_old = my_error(y, x.dot(result)), cost_new\n",
    "        i += 1\n",
    "    print(\"Iterations: {}\".format(i))\n",
    "    return result\n",
    "\n",
    "beta_gradient = linear_regression_gradient_train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we don't have to use any numerical optimization method for linear regression, because we can find a closed form solution for our minimization problem:\n",
    "\n",
    "$$\\operatorname{argmin}_β \\frac{1}{n} \\|Xβ−y\\|_2^2 = \\frac{1}{n}(β^t X^t X β − 2β^t X^t y + y^t y)$$\n",
    "\n",
    "The gradient is given by\n",
    "\n",
    "$$\\nabla_β \\frac{1}{n} \\|Xβ−y\\|_2^2 = \\frac{2}{n}(X^t X β − X^t y).$$\n",
    "\n",
    "This immediately yields\n",
    "\n",
    "$$β = (X^t X)^{−1} X^t y.$$\n",
    "\n",
    "In general, $X^t X$ must not be invertible. This is particularly the case if the number of features is high compared to the sample size. In such a case the equation for $β$ admits several solutions. One of them can be found by replacing the inverse matrix by a generalized inverse/pseudoinverse $X^+$. This leads to the solution\n",
    "$$β = X^+ y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-efc1d35056ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbeta_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression_normal_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-efc1d35056ab>\u001b[0m in \u001b[0;36mlinear_regression_normal_train\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_regression_normal_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbeta_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression_normal_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(a, rcond)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m     \u001b[0;31m# discard small singular values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_svd_nonconvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_lstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "def linear_regression_normal_train(x, y):\n",
    "    result = np.linalg.pinv(x).dot(y)\n",
    "    return result\n",
    "\n",
    "beta_normal = linear_regression_normal_train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the linear regression by using the package scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression(fit_intercept=False, normalize=False).fit(x_train, y_train)\n",
    "\n",
    "beta_sklearn = np.transpose(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments:\n",
    "* scikit-learn treats machine learning algorithms as classes. For example, .fit is a method of the class LinearRegression.\n",
    "* All derived information are stored in variables with a trailing underscore by scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our different algorithms perform!\n",
    "\n",
    "First of all, we need to write a function that from a vector of inputs and a vector of estimated coefficients, $\\beta$, can return a vector of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_predict(x, beta):\n",
    "    result = x.dot(beta)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------Gradient----------\")\n",
    "print(beta_gradient)\n",
    "print(\"----------Normal----------\")\n",
    "print(beta_normal)\n",
    "print(\"----------Sklearn----------\")\n",
    "print(beta_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"----------Gradient----------\")\n",
    "print(\"Training error: \", my_error(y_train, linear_regression_predict(x_train, beta_gradient)))\n",
    "print(\"Test error: \", my_error(y_test, linear_regression_predict(x_test, beta_gradient)))\n",
    "\n",
    "print(\"----------Normal----------\")\n",
    "print(\"Training error: \", my_error(y_train, linear_regression_predict(x_train, beta_normal)))\n",
    "print(\"Test error: \", my_error(y_test, linear_regression_predict(x_test, beta_normal)))\n",
    "\n",
    "print(\"----------Sklearn----------\")\n",
    "print(\"Training error: \", my_error(y_train, linear_regression_predict(x_train, beta_sklearn)))\n",
    "print(\"Test error: \", my_error(y_test, linear_regression_predict(x_test, beta_sklearn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test - linear_regression_predict(x_test, beta_sklearn), 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test - linear_regression_predict(x_test, beta_gradient), 'ro',\n",
    "        y_test - linear_regression_predict(x_test, beta_normal), 'bo',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this learning?\n",
    "* No iterative learning steps in normal equation does not mean that the algorithm is not learning.\n",
    "* Linear regression profits a lot from its easy form. This makes the algorithm explainable.\n",
    "\n",
    "Normal equation vs gradient descent\n",
    "* Calculating inverse matrix is difficult for many features. Updating rule stays easy.\n",
    "* Learning rate has to be chosen well for gradient descent to work. So the learning algorithm gets more difficult. --> See also Class 4\n",
    "\n",
    "Theoretical bounds\n",
    "* Consider a data set $(x^{(i)},y_i)_{i=1,…,n}$ following the model $y = x^t β + ε$ for i.i.d. noise $ε$ with mean 0 and variance $σ^2$.\n",
    "* The training error satisfies $\\mathbb{E}[R(β)] = σ^2 (1−\\frac{d+1}{n_{test}})$.\n",
    "* The test error satisfies $\\mathbb{E}[R(β)] = σ^2 (1+\\frac{d+1}{n_{train}})$.\n",
    "* This shows that the model is indeed learning some information from the noise as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the code yourself! Possible ideas that might lead you to interesting observations are:\n",
    "1. Try your algorithms on the new dataset diabetes from scikit-learn.\n",
    "2. Implement the normalization. How do the results of your algorithms change?\n",
    "3. Implement the algorithm for the absolute value loss function. Compare the estimator for the absolute value loss function with the one for the squared error.\n",
    "4. Try your algorithm on a data set you generated yourself: Fix some $β∈ℝ^d$ with possibly some parameters being (close to) zero. Generate some samples via $y = \\sum_{i=1}^d β_i x^{(i)} + ε$ with some i.i.d. noise $ε$.\n",
    "5. Plot the training error and the test error for your model trained on smaller datasets of size $m\\le n$ for different $m$. As $m$ increases, what can you see?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
